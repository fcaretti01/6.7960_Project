<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
      background-color: #f9f9f9;
      color: #333;
	  line-height: 2;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    	max-width: 1100px; /* Optional: Maximum width */
		background-color:#fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: center;
			/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir"; */
			padding: 5px;
	}
	.margin-right-block {
			/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir"; */
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	/* Metadata Section */
	.metadata {
    display: flex;
    justify-content: center; /* Centers the content horizontally */
    align-items: center; /* Centers the content vertically */
    gap: 100px; /* Adds spacing between items */
    margin-bottom: 20px;
    font-size: 0.9em;
    color: #555;
    text-align: center; /* Ensures text is centered within each item */
	}
	.metadata div {
		text-align: center; /* Centers text inside each metadata section */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
	font-family: Verdana, Geneva, Tahoma, sans-serif;
    flex-grow: 1;
	width: 100%;
    max-width: calc(100% - 290); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
		text-align: center;
		font-weight: bold;
		line-height: 1.5;
	}

	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>caretti_ghitturi_6.7960_final_proj</title>
      <meta property="og:title" content="Decoding Multi-Head Attention for High-Frequency Financial Forecasting" />
			<meta charset="UTF-8">
  </head>

  <body>

	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<table class="header" align=center>
					<tr>
						<td colspan=1>
							<span style="font-size: 32px; /* Adds fallbacks */">Decoding Multi-Head Attention for Intra-Day Financial Time-Series Forecasting</span>
						</td>
					<tr>
						<td colspan=1 align=center><span style="font-size:18px"><br>Final project for 6.7960, MIT, Fall 2024</span></td>
					</tr>
			</table>
		</div>
		<div class="margin-right-block">
		</div>
		</div>
	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<div class="metadata">
				<!-- Metadata Section -->
					<div>
					<h4>Authors</h4>
					<p>Filippo Caretti</p>
					<p>Ludovico Ghitturi</p>
					</div>
					<div>
						<h4>emails</h4>
						<p>fcaretti@mit.edu</p>
						<p>ludo25@mit.edu</p>
					</div>
					<div>
					<h4>Affiliations</h4>
					<p>MIT</p>
					<p>MIT</p>
					</div>
				</div>
		</div>
		<div class="margin-right-block">
		</div>	  
	</div>

	<div class="content-margin-container" id="intro">
			<div class="margin-left-block">
		<!-- table of contents here -->
		<div style="position:fixed; max-width:inherit; top:max(20%,120px)">
			<b style="font-size:16px">Outline</b><br><br>
			<a href="#intro">Introduction</a><br><br>
			<a href="#background">Background</a><br><br>
			<a href="#implications_and_limitations">Implications and limitations</a><br><br>
		</div>
			</div>
		<div class="main-content-block">
		<!--You can embed an image like this:-->
		<img src="./images/mit_dome.webp" width=512px/>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
						Transformers have demonstrated remarkable predictive capabilities, especially in tasks requiring the identification and prioritization of relevant features through their sophisticated attention mechanisms. Their ability to capture complex dependencies in sequential data has made them a cornerstone in modern machine learning. Despite these successes, the interpretability of transformer-based models remains an open challenge. While these models often produce accurate predictions, the rationale behind their decisions is often opaque, leaving practitioners with limited insights into the factors driving their outputs.

						In this work, we focus on unraveling the decision-making processes of transformers in the context of intraday financial time series prediction. Specifically, we aim to analyze how transformers utilize lagged input features to forecast financial metrics at high-frequency intervals. By examining the internal workings of a GPT-2 small transformer, we seek to mechanistically understand how the architecture implements an autoregressive prediction task in this domain. Our investigation centers on identifying the pathways and components within the model that are most influential in shaping its predictions.
						
						To achieve this, we draw inspiration from the systematic approach introduced by Wang et al. (2022), which uses circuit analysis and causal interventions to trace the flow of information within deep learning models. This methodology, known as "path patching," allows for the iterative identification of critical components by intervening on the model's activations and observing the impact on its outputs. By applying this framework to the logits and intermediate activations of GPT-2, we aim to isolate and interpret the contribution of individual model components and layers to the overall prediction.
						
						Through this analysis, our goal is to bridge the gap between prediction accuracy and interpretability, offering a more transparent understanding of how transformers process and prioritize input features in a complex financial prediction setting.
		    </div>
		    <div class="margin-right-block">
						
		    </div>
		</div>

		<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Background</h1>
				Transformers have revolutionized sequence modelling becoming the industry standard for many prediciton tasks. The strength of this architecture comes from implementing the attention mechanisms, which is able to process global information efficiently by focusing only on the signal that is most salient for task at hand. In our context, we are interested in a particular kind of attention based on the idea of queries, keys and vlaues. In query-key-value attention, each token is associated with a query vector, a key vector, and a value vector. We have then that for a token t: <br><br>

          
          <center>
			<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>q</mi>
				  <mo>=</mo>
				  <msub>
					<mi>W</mi>
					<mi>q</mi>
				  </msub>
				  <mi>t</mi>
				</mrow>
			  </math>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>k</mi>
				  <mo>=</mo>
				  <msub>
					<mi>W</mi>
					<mi>k</mi>
				  </msub>
				  <mi>t</mi>
				</mrow>
			  </math>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>v</mi>
				  <mo>=</mo>
				  <msub>
					<mi>W</mi>
					<mi>v</mi>
				  </msub>
				  <mi>t</mi>
				</mrow>
			  </math>  
          </center>
          <br>
          Every token will then submit a query vector which will then be compared to to the keys vector via a measure of similarity (dot product) <br><br>
		  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
			<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>s</mi>
				  <mo>=</mo>
				  <mfenced open=&#x5B; close=&#x5D;>
					<mrow>
						<mo>[</mo>
						<mi>s</mi>
						<msub>
							<mi></mi>
							<mn>1</mn>
						</msub>
					  <mo>,</mo>
					  <mo>&#x2026;</mo>
					  <mo>,</mo>
						<mi>s</mi>
						<msub>
							<mi></mi>
							<mn>N</mn>
						</msub>
					  <mo>]</mo>
					</mrow>
				  </mfenced>
				  <msup>
					<mo></mo>
					<mo>T</mo>
				  </msup>
				  <mo>=</mo>
				  <mfenced open="[" close="]">
					<mrow>
					  <mfenced open="[" close="]">
						<mtext>[</mtext>
						<mrow>
							<mi>q</mi>
							<msup>
								<mi></mi>
								<mtext>T</mtext>
							</msup>
							<msub>
								<mi></mi>
								<mtext>input</mtext>
							</msub>
					
						  <msub>
							<mi>k</mi>
							<mn>1</mn>
						  </msub>
						</mrow>
					  </mfenced>
					  <mo>,</mo>
					  <mo>&#x2026;</mo>
					  <mo>,</mo>
					  <mfenced open="[" close="]">
						<mrow>
						  <msub>
							<msup>
								<mi>q</mi>
								<mtext>T</mtext>
							</msup>
							<mtext>input</mtext>
						  </msub>
						  <msub>
							<mi>k</mi>
							<mi>N</mi>
						  </msub>
						</mrow>
					  </mfenced>
					</mrow>
				  </mfenced>
				  <mtext>]</mtext>
				  <msup>
					<mo></mo>
					<mo>T</mo>
				  </msup>
				</mrow>
			  </math>

			We then normalize the vector of similarities with a softmax and weight the value matrix using the attention weights (rescaled similarities)

			<br><br>
			  
		    </div>


		    <div class="margin-right-block" style="transform: translate(0%, -50%);"> <!-- you can move the margin notes up and down with translate -->
          Adapted from lecture notes on Transformers architecture <a href="#ref_1">[1]</a>.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Another section</h1>
            In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video>
		    </div>
		    <div class="margin-right-block">
					A caption for the video could go here.
		    </div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Implications and limitations</h1>
						Let's end with some discussion of the implications and limitations.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
							<a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>

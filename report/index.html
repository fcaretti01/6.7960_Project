<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
      background-color: #f9f9f9;
      color: #333;
	  line-height: 2;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    	max-width: 1100px; /* Optional: Maximum width */
		background-color:#fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}	
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: center;
			/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir"; */
			padding: 5px;
	}
	.margin-right-block {
			/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir"; */
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	/* Metadata Section */
	.metadata {
    display: flex;
    justify-content: center; /* Centers the content horizontally */
    align-items: center; /* Centers the content vertically */
    gap: 100px; /* Adds spacing between items */
    margin-bottom: 20px;
    font-size: 0.9em;
    color: #555;
    text-align: center; /* Ensures text is centered within each item */
	}
	.metadata div {
		text-align: center; /* Centers text inside each metadata section */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
	font-family: Verdana, Geneva, Tahoma, sans-serif;
    flex-grow: 1;
	width: 100%;
    max-width: calc(100% - 290); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
		text-align: center;
		font-weight: bold;
		line-height: 1.5;
	}

	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>caretti_ghitturi_6.7960_final_proj</title>
      <meta property="og:title" content="Decoding Multi-Head Attention for High-Frequency Financial Forecasting" />
			<meta charset="UTF-8">
  </head>

  <body>

	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<table class="header" align=center>
					<tr>
						<td colspan=1>
							<span style="font-size: 32px; /* Adds fallbacks */">Decoding Multi-Head Attention for Intra-Day Financial Time-Series Forecasting</span>
						</td>
					<tr>
						<td colspan=1 align=center><span style="font-size:18px"><br>Final project for 6.7960, MIT, Fall 2024</span></td>
					</tr>
			</table>
		</div>
		<div class="margin-right-block">
		</div>
		</div>
	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<div class="metadata">
				<!-- Metadata Section -->
					<div>
					<h4>Authors</h4>
					<p>Filippo Caretti</p>
					<p>Ludovico Ghitturi</p>
					</div>
					<div>
						<h4>emails</h4>
						<p>fcaretti@mit.edu</p>
						<p>ludo25@mit.edu</p>
					</div>
					<div>
					<h4>Affiliations</h4>
					<p>MIT</p>
					<p>MIT</p>
					</div>
				</div>
		</div>
		<div class="margin-right-block">
		</div>	  
	</div>

	<div class="content-margin-container" id="intro">
			<div class="margin-left-block">
		<!-- table of contents here -->
		<div style="position:fixed; max-width:inherit; top:max(20%,120px)">
			<b style="font-size:16px">Outline</b><br><br>
			<a href="#intro">Introduction</a><br><br>
			<a href="#background">Background</a><br><br>
			<a href="#gpt2">GPT-2 Architecture</a><br><br>
			<a href="#task">Task Definition</a><br><br>
			<a href="#net_analysis">Network Analysis</a><br><br>
			<a href="#implementation">Implementation</a><br><br>
			<a href="#results">Results</a><br><br>
			<a href="#conclusion">Conclusion</a><br><br>
		</div>
			</div>
		<div class="main-content-block">
		<!--You can embed an image like this:-->
		<img src="./images/mit_dome.webp" width=512px/>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
						Transformers have demonstrated remarkable predictive capabilities, especially in tasks requiring the identification and prioritization of relevant features through their sophisticated attention mechanisms. Their ability to capture complex dependencies in sequential data has made them a cornerstone in modern machine learning. Despite these successes, the interpretability of transformer-based models remains an open challenge. While these models often produce accurate predictions, the rationale behind their decisions is often opaque, leaving practitioners with limited insights into the factors driving their outputs.

						In this work, we focus on unraveling the decision-making processes of transformers in the context of intraday financial time series prediction. Specifically, we aim to analyze how transformers utilize lagged input features to forecast financial metrics at high-frequency intervals. By examining the internal workings of a GPT-2 small transformer, we seek to mechanistically understand how the architecture implements an autoregressive prediction task in this domain. Our investigation centers on identifying the pathways and components within the model that are most influential in shaping its predictions.
						
						To achieve this, we draw inspiration from the systematic approach introduced by Wang et al. (2022), which uses circuit analysis and causal interventions to trace the flow of information within deep learning models. This methodology, known as "path patching," allows for the iterative identification of critical components by intervening on the model's activations and observing the impact on its outputs. By applying this framework to the logits and intermediate activations of GPT-2, we aim to isolate and interpret the contribution of individual model components and layers to the overall prediction.
						
						Through this analysis, our goal is to bridge the gap between prediction accuracy and interpretability, offering a more transparent understanding of how transformers process and prioritize input features in a complex financial prediction setting.
		    </div>
		    <div class="margin-right-block">
						
		    </div>
		</div>

		<div class="content-margin-container" id="background">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
				<h1>Background</h1>
			Transformers have revolutionized sequence modeling, becoming the industry standard for many prediction tasks. Their strength lies in their ability to process global information efficiently, focusing only on the most relevant signals for a given task through attention mechanisms. At the heart of this architecture is the self-attention mechanism, which allows each token in a sequence to dynamically determine its relationship with all other tokens. 

			A core building block of self-attention is the concept of queries, keys, and values. Each token in the input sequence is transformed into three vectors: a query vector, a key vector, and a value vector. These are computed as follows for a token \( t \):
			<br><br>
	  
			<center>
			<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>q</mi>
				  <mo>=</mo>
				  <msub>
					<mi>W</mi>
					<mi>q</mi>
				  </msub>
				  <mi>t</mi>
				</mrow>
			  </math>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>k</mi>
				  <mo>=</mo>
				  <msub>
					<mi>W</mi>
					<mi>k</mi>
				  </msub>
				  <mi>t</mi>
				</mrow>
			  </math>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>v</mi>
				  <mo>=</mo>
				  <msub>
					<mi>W</mi>
					<mi>v</mi>
				  </msub>
				  <mi>t</mi>
				</mrow>
			  </math>  
			</center>
			<br>

			The query vector represents the "question" a token asks about the sequence, while the key vectors encode the "context" or "relevance" of other tokens. The values represent the information each token contributes. To compute self-attention, the query of a token is compared against the keys of all tokens in the sequence using a similarity measure (dot product). This produces a vector of attention scores:
			<br><br>

			<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>s</mi>
				  <mo>=</mo>
				  <mfenced open="[" close="]">
					<mrow>
					  <mi>q</mi>
					  <msup>
						<mi>k</mi>
						<mo>T</mo>
					  </msup>
					</mrow>
				  </mfenced>
				</mrow>
			</math>
			<br>

			These attention scores are then normalized using a softmax function, converting them into probabilities that indicate the relative importance of each token:
			<br><br>

			<center>
				<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>a</mi>
				  <mo>=</mo>
				  <mi>softmax</mi>
				  <mo>(</mo>
				  <mi>s</mi>
				  <mo>)</mo>
				</mrow>
			  </math>
			  <br><br>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>output</mi>
				  <mo>=</mo>
				  <mi>a</mi>
				  <mi>v</mi>
				</mrow>
			  </math>
			</center>
			<br>

			The self-attention mechanism combines the values weighted by their attention probabilities, producing an output vector for each token that incorporates information from the entire sequence. This approach allows the model to capture both local and global dependencies, which is particularly powerful for tasks like machine translation and time-series prediction.

			Unlike traditional sequence models like recurrent neural networks (RNNs), self-attention does not impose a sequential constraint. Instead, it enables direct access to all tokens simultaneously, making it more efficient for parallel computation and better at handling long-range dependencies.

			In the following sections, we will explore how these principles underpin transformer architectures like GPT-2, focusing on their applications to complex prediction tasks in financial time series analysis.
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
			Adapted from lecture notes on Transformers architecture <a href="#ref_1">[1]</a>.
		</div>
		</div>
		<div class="content-margin-container" id="gpt2">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h4>GPT-2 Architecture</h4>

			For the prediction task we are trying to analyze we chose GPT-2 architecture, which builds upon the self-attention mechanism with a stack of transformer decoder blocks. Each block consists of the following key components:

			<ol>
				<li>
					<b>Masked Self-Attention:</b> GPT-2 uses masked self-attention to ensure that predictions for a token depend only on the tokens that precede it. This masking is achieved by setting the attention weights to zero for future tokens.
				</li>
				<li>
					<b>Feedforward Neural Network (FFN):</b> Each attention block is followed by a position-wise fully connected feedforward network with non-linear activation functions (ReLU or GELU). This component helps in learning complex mappings.
				</li>
				<li>
					<b>Layer Normalization:</b> Layer normalization is applied before the attention and feedforward layers, improving training stability and convergence.
				</li>
				<li>
					<b>Residual Connections:</b> Residual connections around each sub-layer (attention and feedforward) allow the model to effectively propagate gradients during training.
				</li>
			</ol>

			The input to GPT-2 is first embedded into a high-dimensional space using a token embedding layer and a positional encoding to provide the model with information about the order of tokens in the sequence. The embeddings are then passed through the stack of decoder blocks.

			<h4>GPT-2 for Autoregressive Prediction</h4>

			GPT-2 is trained in an autoregressive manner, predicting the next token in the sequence given all previous tokens. This is achieved by minimizing the negative log-likelihood of the predicted tokens during training. The model's autoregressive nature aligns well with financial time series prediction, where future values depend on past observations.

			By using multiple attention heads in each self-attention layer, GPT-2 can attend to different aspects of the input sequence simultaneously. This multi-head attention mechanism enables the model to learn diverse representations that are crucial for capturing the nuances of high-frequency financial data.

			In the next section, we will detail how we analyze the components of GPT-2 and their contribution to predictions in the context of financial time series data, utilizing the path patching framework inspired by Wang et al. (2022).
		    
			</div>
			<div class="margin-right-block" style="transform: translate(0%, -10%);"> 
				link to paper <a href="#papergpt2">[3]</a>
			</div>
		</div>
	

			
		<div class="content-margin-container" id="task">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h1>Task definition</h1><p>
					We address an autoregressive prediction problem in the context of intraday financial markets. The objective is to predict the sign of the return 
					(<math xmlns="http://www.w3.org/1998/Math/MathML">
						<mrow>
							<mo>{</mo>
							<mn>1</mn>
							<mo>,</mo>
							<mn>0</mn>
							<mo>,</mo>
							<mo>-</mo>
							<mn>1</mn>
							<mo>}</mo>
						</mrow>
					</math>)
					for 30-minute intervals using a set of lagged input features. The task is framed as a three-class classification problem. To facilitate this, 
					we construct a comprehensive set of technical indicators derived from historical data, including log returns, volatility, and trading volume, 
					along with their lagged values. These features serve as the predictive inputs to the model. Below, we provide a schematic visualization of the input 
					representation and the prediction task.
				</p>
				<p>
					Below a visual representation of our model, where <b>X</b>, <b>Y</b>, <b>Z</b> are <i>(T x 1)</i> vectors, where <i>T</i> is our lookback period.
				</p>
				<center>
					<svg width="800" height="500" xmlns="http://www.w3.org/2000/svg">

						<!-- Connections -->
						<line x1="170" y1="150" x2="300" y2="150" stroke="gray" stroke-width="2" />
						<line x1="170" y1="250" x2="300" y2="250" stroke="gray" stroke-width="2" />
						<line x1="170" y1="350" x2="300" y2="350" stroke="gray" stroke-width="2" />
				
						<line x1="450" y1="150" x2="650" y2="250" stroke="gray" stroke-width="2" />
						<line x1="450" y1="250" x2="650" y2="250" stroke="gray" stroke-width="2" />
						<line x1="450" y1="350" x2="650" y2="250" stroke="gray" stroke-width="2" />
						
						<!-- Title -->
						<text x="290" y="30" font-size="18" font-family="Arial">Autoregressive Prediction</text>
						
						<!-- Input Layer -->
						<text x="70" y="100" font-size="16" font-family="Arial">Input Layer (Features)</text>
						
						<!-- Input Nodes -->
						<circle cx="150" cy="150" r="20" fill="lightgreen" />
						<text x="150" y="150" font-size="20" font-family="Arial" text-anchor="middle" dy=".3em">
							<tspan font-weight="bold">X</tspan>
						</text>
				
						<circle cx="150" cy="250" r="20" fill="lightgreen" />
						<text x="150" y="250" font-size="20" font-family="Arial" text-anchor="middle" dy=".3em">
							<tspan font-weight="bold">Y</tspan>
						</text>
				
						<circle cx="150" cy="350" r="20" fill="lightgreen" />
						<text x="150" y="350" font-size="20" font-family="Arial" text-anchor="middle" dy=".3em">
							<tspan font-weight="bold">Z</tspan>
						</text>
						
						<!-- GPT-2 Block -->
						<text x="275" y="100" font-size="16" font-family="Arial">Network (GPT-2 based architecture)</text>
						<rect x="300" y="120" width="150" height="300" fill="#d9e8ff" stroke="black" />
						<text x="375" y="270" font-size="14" font-family="Arial" text-anchor="middle">Network</text>
						
						<!-- Output Layer -->
						<text x="600" y="100" font-size="16" font-family="Arial">Output Layer</text>
						
						<!-- Output Node -->
						<circle cx="650" cy="250" r="20" fill="lightcoral" />
						<text x="650" y="250" font-size="20" font-weight="bold" font-family="Arial" text-anchor="middle" dy=".3em">Prediction</text>
						

					</svg>
				</center>
				<p>
					To implement our model, we adopted the <em>GPT-2 architecture</em>, tailored to the requirements of our autoregressive prediction task. Specifically, we initialized our model to accept input features shaped as 
					<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
						<mi>(B, T, N)</mi>
					</math>, 
					where 
					<em>B</em> represents the batch size, 
					<em>T</em> denotes the lookback period 
					(<math xmlns="http://www.w3.org/1998/Math/MathML"><mn>43</mn></math>, corresponding to three days of intraday data), 
					and <em>N</em> specifies the number of feature dimensions 
					(<math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn></math>, including lagged values of volatility, log returns, and volume).
				</p>
				<p>
					The input features were first processed through an <em>embedding layer</em> that projected the raw data into a higher-dimensional space, enabling the model to capture more intricate patterns in the data. To incorporate temporal information, a 
					<em>positional embedding layer</em> was added to encode the sequential nature of the time series, ensuring that the model could differentiate between input tokens based on their relative positions.
				</p>
				<p>
					The core of the network consists of three stacked <em>self-attention blocks</em>, adhering to the principles of the GPT-2 architecture. Each block includes:
					<ul>
						<li>A <em>multi-head self-attention mechanism</em> to capture dependencies across all lagged input features.</li>
						<li>A fully connected <em>feedforward network (MLP)</em> to process intermediate representations.</li>
						<li><em>Residual connections</em> with layer normalization to facilitate gradient flow and improve model stability during training.</li>
					</ul>
					These components work synergistically to extract meaningful relationships and patterns from the input data.
				</p>
				<p>
					In the final stage, the output of the self-attention blocks is passed through a <em>decoding layer</em> designed for classification. The decoder transforms the processed representations into predictions corresponding to one of the three target classes 
					(<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">{<mn>1</mn>, <mn>0</mn>, <mn>-1</mn>}</math>), 
					representing the sign of the return. This setup ensures that the model is well-suited for the autoregressive classification task, leveraging the strengths of the GPT-2 architecture for financial time series data.
				</p>
				<p>
					Below the visualization of the initialized model:
				</p>
				<img src="./images/GPT-2 Architecture.png" width=512px/>
			</div>
			<div class="margin-right-block" style="transform: translate(0%, -10%);">
			</div>
		</div>

		<!-- NETWORK ANALYSIS PART  -->
		<div class="content-margin-container" id="net_analysis">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h1><p>Network Analysis</p></h1>
				
				  <p>
					Having defined the model we aim to train, the next step involves delving into its inner workings to make the model more interpretable and human-comprehensible. Our approach draws inspiration from the work of Wang et al. (2022), which introduces a mechanistic framework for understanding neural networks. One of the key concepts proposed is the <em>hookpoint</em>. A hookpoint is a custom pytorch <code>nn.Module</code> that operates as a passthrough immediately after every activation function in the model, returning the identity of the data it processes, implementing either forward or backward hooks. These hooks are instrumental in caching and analyzing the activations at various points in the model, particularly within attention layers, as well as conducting ablation studies.
				  </p>
				
				  <p>
					In our experiments, we leverage hooks to silence the outputs of attention heads selectively, aiming to identify which heads most significantly influence the model's predictions. This process employs a technique called <strong>mean ablation</strong>, where the output of a single head is replaced with its mean value. By iteratively deactivating different attention heads and observing changes in the loss, we can quantify the relative importance of each head. The comparison between loss variations enables us to rank heads in terms of their contribution to the prediction process.
				  </p>
				
				  <p>
					After identifying the most influential heads, we analyze their outputs to determine the temporal dependencies the model emphasizes. This allows us to uncover which time steps the model focuses on most heavily during its predictions. Such insights not only improve our understanding of the model's decision-making process but also provide a pathway to refine its interpretability and trustworthiness.
				  </p>
				
			</div>
			<div class="margin-right-block"></div>
		</div>

		<div class="content-margin-container" id="implementation">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h1> Implementation </h1>
				<p>
					We structured our implementation by dividing the process into four key steps: <b>1) Data Collection & Preparation</b>; <b>3) Model Training</b>; and <b>4) Network Analysis</b>.
				  </p>
				
				  <h4>1. Data Collection & Preparation</h4>
				  <p>
					In the initial phase, we sourced financial data from <a href="https://polygon.io" target="_blank">Polygon.io</a> by developing a bespoke API. This enabled us to retrieve comprehensive information on all U.S. stocks with 30-minute interval data spanning over five years. The high-resolution dataset provides a robust foundation for subsequent analysis and modeling. 
				  </p>
				  <p>
					Following data extraction, we implemented a series of rigorous data cleaning procedures to address potential issues that could adversely affect model training in subsequent phases. These procedures included the removal of missing values and duplicate entries, as well as the handling of outliers. Missing values were systematically addressed using interpolation techniques, 
					while duplicates were eliminated to maintain dataset integrity. Subsequently, we enriched the dataset by constructing additional features derived from the securities' price movements. Feature engineering focused 
					on key financial metrics such as logarithmic returns, rolling volatilities, and price-volume relationships. These features were selected due to their relevance in capturing temporal and cross-sectional patterns in the data. While the scope of feature engineering was constrained by time limitations, the chosen features provided a robust foundation for predictive modeling. 
					In preparing the dataset for model training, we structured the data into a format suitable for time-series classification tasks. Specifically, each sample in the training dataset was designed as an entry with the shape (<i>T</i>, <i>d</i> ), where <i>T</i> represents the temporal dimension and <i>d</i> denotes the number of features engineered. This configuration ensured
					that the model could effectively leverage both the temporal dynamics and the feature relationships present in the data. Finally, the dataset was partitioned into training and testing subsets to facilitate model evaluation. The temporal structure of the data was preserved during splitting to ensure that the test set consisted of future data relative to the training set, 
					thereby maintaining the chronological integrity necessary for time-series analysis. This division established the foundation for model training and validation in subsequent stages.
				  </p>
				
				  <h4>3. Model Training</h4>
				  <p>
					With the data preprocessed and prepared, we initialized our model based on the architecture outlined above, aiming to elucidate the focus of transformers in a time-series scenario such as the one described. Training was conducted on two distinct models, differing only in that the second model featured an increased dimensionality of weight parameters and an additional attention head (instead of 4). This adjustment was 
					intended to enhance the model's predictive capacity and yield more insightful results. Both models were trained on the MIT High-Performance Computing Cluster, iterating over the training dataset for 20 epochs with a learning rate of <code>1 x 10<sup>-4</sup></code>.
				  </p>
				
				  <h4>4. Network Analysis</h4>
				  <p>
					For the network analysis phase, we employed a mean ablation technique to investigate the contributions of individual attention heads within the transformer architecture. By selectively silencing the output of specific heads, we measured the percentage increase in loss to quantify their importance to the model's predictions. This approach allowed us to identify which heads were most critical for the model's performance, offering valuable insights into how the transformer processes time-series data.
				  </p>
				
			</div>
			<div class="margin-right-block"></div>
		</div>>


		<div class="content-margin-container" id="results">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h1>Results</h1>
				<p>
					We train two variants of the Time Series GPT2-mini model, one with model dimensions of 20 and a 5 dimensional subspace per attention head with 4 attention heads per layer (“small” model), and the other having model dimensionality of 50 (we project each 5-dimensional input token of the input sequence onto a 50-dimensional latent space before initiating the series of Attention layers) and 5 attention heads per layer (“large” model), both of which with a total of 3 attention layers. 
					After training, we carry out the mean ablation studies for each model independently as described above and analyze the head importance defined as the percentage decrease in accuracy when mean ablating each specific head with respect to the baseline model accuracy on a test set. Once we identify the most relevant attention heads that the learned model hinges upon the most for making its predictions we proceed in analyzing the patterns described by such heads via their attention activations on the test set.
					Below we report, in turn, the two main categories of heads we are able to detect with our ablation + attention activation study across the different layers of the model.
					Overall we find, among all heads across the three layers of both networks, that there exist two attention heads that carry the most influence in terms of generating accurate predictions, the first being in the first layer and the second in the third layer of both models. In both models, the relevant head sitting in the first layer of the network consistently focuses, at each time step, on the tokens corresponding to the same time of the day in the previous days, thus paying attention to the time-consistent patterns that repeat across days at regular times. We deem such head the “Time Persistence Head”, capturing a longer-term time-stationary behavior of our series. The second relevant head, located in the third and final attention layer of both the smaller and the larger networks, instead focuses on the most recent past observations at each time step, thus elaborating more short-term intra-day dynamics. We define such head as an “Intra-Day Dynamics Head”.
				</p>
				
				<img src="./images/heat_1.png" width=512px/>
				<img src="./images/heat_2.png" width=512px/>
				<h4>Sum of head importances small model </h4>
				<img src="./images/importance_1.png" width=512px/>
				<h4>Sum of head importances big model </h4>
				<img src="./images/importance_2.png" width=512px/>
				<br><br>
				<h4>Time Persistence Head</h4>
				<p>
					The <b>Time Persistence Head</b> is represented by the fourth head and by the second head in the larger and smaller GPT models respectively, both situated in the first layer of the network. As we can see from figure <a href="#img1">1</a> (larger model) and <a href="#img4">4</a> (smaller model), and especially for the larger model’s head, the attention weights are strongly biased towards a regular pattern that attends to the tokens situated at exactly 14 steps prior to the current token which, in our case of a trading day of 7 hours and thus 14 30min-interval time steps corresponds to an observation realized at exactly the same time of the day in the previous day/s. As the token reaches the end of the sequence, and thus observations for the same time become available not just for the previous day but also for two days prior, we see that the head progressively attends backward in time, always at 14-time steps increments. This indicates that the first high-level feature captured by the model as our sequence enters the network is one that relates to recurring patterns at the same time of the day across trading days, i.e., information that relates to a longer-term possibly mean reverting or trend following behavior of our sequence across days. This is overall one of the strongest structural pattern identified in both networks, which can be seen by the dynamic visualization <a href="#imdim1">gif 1</a> of the top 10 attended tokens for each token in our sequence in the animation below.
				</p>
				<img id="img1" src="./images/1.png" width=512px/>
				<br>
				<img id="imdim1" src="./images/H4L1_second_pattern_model2.gif" alt="Animated GIF Example" style="width: 725px;">
				<br>
				<img id="img2" src="./images/2.png" width=512px/>
				<br><br>
				<p>
					Despite the strength of this attention pattern, however, we saw that for the smaller model this systematic backtracking behavior was hard to achieve, arguably due to the lower dimensional subspace onto which attention operations are carried out by each head, which made it hard to dynamically identify the relevant time steps for each token position, let alone extract information from observations too-far distanced in the past. We therefore see another function emerging in the Time Persistence Head of the smaller model, which is identifying and paying attention to the end-of-day tokens for each time step, irrespectively of where the query time step is located relative to the end-of-day token <a href="#img3">3</a>. In essence, finding and attending to the end-of-day token helps to pin down the absolute time position of each token in the given day, thus playing a similar role of the dynamic Time Persistence Head in capturing repeating behaviors through days at the same time of the day.
					Given the lower model dimensionality of the smaller network, we find that this end-of-day anchoring <a href="#imdim2">gif 2</a> is extremely useful for predictions, as we find it present in many other attention heads and layers beside the Time Persistence Head of the first layer <a href="#img4">4</a>. As this rather strong pattern almost completely disappears in the second, larger model, we attribute this function as one that makes up for the model expressivity and dimensionality needed to capture the time persistent behavior that we find in <a href="#img1">1</a>.
				</p>
				<img id="img3" src="./images/3.png" width=512px/>
				<br>
				<img id="img4" src="./images/4.png" width=512px/>
				<img id="imdim2" src="./images/H2L1-chess_pattern_only in model1.gif" alt="Animated GIF Example" style="width: 725px;">
				<br>
				<br><br>
				<h4> Intra-day Dynamics Head</h4>
				<p>
					The Intra-Day Dynamics Head instead tightly focuses on the past observations closest in time to the current token, thus at all times paying attention to the intra-day behavior of our series. This structure is consistent from the smaller <a href="#img5">5</a> to the larger model <a href="#img6">6</a>, even though the attention pattern of the head in the larger model more strongly focuses on intra-day observations while excluding the end-of-day time steps. This is associated to a more effective behavior of this head since excluding the end-of-day observations, which in the majority of cases correspond to zero returns due to absence of trading (and thus not meaningful information), helps the head focus on more meaningful information unveiled throughout the other time steps of the day. By contrast, the smaller model’s head doesn’t show this behavior and thus uniformly attends to end-of-day observations as well including them in the intra-day dynamics. Both models’ heads show a scheme of progressively decreasing attention weights as the time steps get further back in time relative to the current query token, thus corroborating the hypothesis that such heads are focusing on the latest dynamics with potentially decaying impact in time <a href="#imdim3">gif 3</a>.
				</p>
				<img id="img5" src="./images/5.png" width=512px/>
				<br>
				<img id="img6" src="./images/6.png" width=512px/>
				<br>
				<img id="imdim3" src="./images/avg model2 H2L3 second pattern.gif" alt="Animated GIF Example" style="width: 725px;">
				<br>
			</div>
			<div class="margin-right-block"></div>
		</div>>


		<!-- Conclusion -->
		<div class="content-margin-container" id="conclusion">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h1>Conclusion</h1>
				<p>
					This study overall investigates and sheds light on the behavior of multi-head attention in a Transformer Decoder model applied to a time-series setting. In addition, our empirical results present information on the role of dimensionality of the vector subspace onto which we project our input tokens and apply attention operations for each head. Through our mean ablation experiments on the activation of each attention head in each layer of our models we identified two main classes of heads dominating both the smaller and the larger of the trained models. These are a Time Persistence Head, focusing on the same times of the day in previous days and thus capturing longer-term dynamics of our sequence, and an Intra-Day Dynamics Head, focusing on the more recent time observations available at any given point in time and thus providing information on short-term  dynamics of our sequence. They are respectively located in the first and third layer of our networks, suggesting, in line and analogy with the interpretation of depth in computer vision models, that earlier layers (and heads) focus on the extraction of higher-level features while later layers in the network refine the representation of each input in its detail, which in our time series setting turn out to be intra-day details or information.
					These results are significant in that, to our knowledge, no previous study analyzed the behavior of multi-head self-attention mechanics on high-frequency time series settings, and they thus inform us on the information processing capabilities of the attention technology also on the time series domain, with a particular importance of attention head specialization paving the way for more interpretability studies.
					The second important finding is the relevance of the dimensionality of the latent space onto which attention operations are carried out, with a higher dimensionality crucially extending the model’s expressivity and its ability to capture more detailed and potentially dynamically changing patterns. This is represented by the two patterns highlighted above being significantly strengthened when the trained model goes from a dimensionality of 5 to one of 10 for the key, query, and value vectors.
				</p>
				<p>	
					Further studies could interestingly focus on investigating the role of other modules of the transformer architecture, most notably pointwise MLPs, which in NLP domains have been shown to encode the majority of the knowledge stored and retrieved by Large Language Models during inference. In addition looking at the impact of network might also be interesting, as new patterns might arise as the complexity of model scales.
					It would thus be interesting to understand which analogies might be present in the time series setting.
				</p>
			</div>
			<div class="margin-right-block"></div>
		</div>



		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							 
							<a href="https://arxiv.org/abs/2211.00593">[1]</a> Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt. 
							*Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 Small*. 
							Redwood Research, UC Berkeley
							<br>

							<a href="https://arxiv.org/abs/2202.05262">[2] </a>Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov. 
								*Locating and Editing Factual Associations in GPT*. 
								Submitted on 10 Feb 2022 (v1), last revised 13 Jan 2023 (this version, v5).

							<p id="papergpt2">
							<a href="https://arxiv.org/pdf/2009.04968">[3]</a>Montesinos, D. M. (2020). Modern methods of text generation. National Research University Higher School of Economics, Moscow, Russia.
							</p>
							<br><br>
						</div>
		    </div>	
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>

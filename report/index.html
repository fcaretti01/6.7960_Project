<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
      background-color: #f9f9f9;
      color: #333;
	  line-height: 2;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    	max-width: 1100px; /* Optional: Maximum width */
		background-color:#fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: center;
			/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir"; */
			padding: 5px;
	}
	.margin-right-block {
			/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir"; */
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	/* Metadata Section */
	.metadata {
    display: flex;
    justify-content: center; /* Centers the content horizontally */
    align-items: center; /* Centers the content vertically */
    gap: 100px; /* Adds spacing between items */
    margin-bottom: 20px;
    font-size: 0.9em;
    color: #555;
    text-align: center; /* Ensures text is centered within each item */
	}
	.metadata div {
		text-align: center; /* Centers text inside each metadata section */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
	font-family: Verdana, Geneva, Tahoma, sans-serif;
    flex-grow: 1;
	width: 100%;
    max-width: calc(100% - 290); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
		text-align: center;
		font-weight: bold;
		line-height: 1.5;
	}

	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>caretti_ghitturi_6.7960_final_proj</title>
      <meta property="og:title" content="Decoding Multi-Head Attention for High-Frequency Financial Forecasting" />
			<meta charset="UTF-8">
  </head>

  <body>

	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<table class="header" align=center>
					<tr>
						<td colspan=1>
							<span style="font-size: 32px; /* Adds fallbacks */">Decoding Multi-Head Attention for Intra-Day Financial Time-Series Forecasting</span>
						</td>
					<tr>
						<td colspan=1 align=center><span style="font-size:18px"><br>Final project for 6.7960, MIT, Fall 2024</span></td>
					</tr>
			</table>
		</div>
		<div class="margin-right-block">
		</div>
		</div>
	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<div class="metadata">
				<!-- Metadata Section -->
					<div>
					<h4>Authors</h4>
					<p>Filippo Caretti</p>
					<p>Ludovico Ghitturi</p>
					</div>
					<div>
						<h4>emails</h4>
						<p>fcaretti@mit.edu</p>
						<p>ludo25@mit.edu</p>
					</div>
					<div>
					<h4>Affiliations</h4>
					<p>MIT</p>
					<p>MIT</p>
					</div>
				</div>
		</div>
		<div class="margin-right-block">
		</div>	  
	</div>

	<div class="content-margin-container" id="intro">
			<div class="margin-left-block">
		<!-- table of contents here -->
		<div style="position:fixed; max-width:inherit; top:max(20%,120px)">
			<b style="font-size:16px">Outline</b><br><br>
			<a href="#intro">Introduction</a><br><br>
			<a href="#background">Background</a><br><br>
			<a href="#gpt2">GPT-2 Architecture</a><br><br>
			<a href="#task">Task Definition</a><br><br>
			<a href="#net_analysis">Network Analysis</a><br><br>
			<a href="#implementation">Implementation</a><br><br>
			<a href="#results">Results</a><br><br>
			<a href="#implications_and_limitations">Implications and limitations</a><br><br>
		</div>
			</div>
		<div class="main-content-block">
		<!--You can embed an image like this:-->
		<img src="./images/mit_dome.webp" width=512px/>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
						Transformers have demonstrated remarkable predictive capabilities, especially in tasks requiring the identification and prioritization of relevant features through their sophisticated attention mechanisms. Their ability to capture complex dependencies in sequential data has made them a cornerstone in modern machine learning. Despite these successes, the interpretability of transformer-based models remains an open challenge. While these models often produce accurate predictions, the rationale behind their decisions is often opaque, leaving practitioners with limited insights into the factors driving their outputs.

						In this work, we focus on unraveling the decision-making processes of transformers in the context of intraday financial time series prediction. Specifically, we aim to analyze how transformers utilize lagged input features to forecast financial metrics at high-frequency intervals. By examining the internal workings of a GPT-2 small transformer, we seek to mechanistically understand how the architecture implements an autoregressive prediction task in this domain. Our investigation centers on identifying the pathways and components within the model that are most influential in shaping its predictions.
						
						To achieve this, we draw inspiration from the systematic approach introduced by Wang et al. (2022), which uses circuit analysis and causal interventions to trace the flow of information within deep learning models. This methodology, known as "path patching," allows for the iterative identification of critical components by intervening on the model's activations and observing the impact on its outputs. By applying this framework to the logits and intermediate activations of GPT-2, we aim to isolate and interpret the contribution of individual model components and layers to the overall prediction.
						
						Through this analysis, our goal is to bridge the gap between prediction accuracy and interpretability, offering a more transparent understanding of how transformers process and prioritize input features in a complex financial prediction setting.
		    </div>
		    <div class="margin-right-block">
						
		    </div>
		</div>

		<div class="content-margin-container" id="background">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
				<h1>Background</h1>
			Transformers have revolutionized sequence modeling, becoming the industry standard for many prediction tasks. Their strength lies in their ability to process global information efficiently, focusing only on the most relevant signals for a given task through attention mechanisms. At the heart of this architecture is the self-attention mechanism, which allows each token in a sequence to dynamically determine its relationship with all other tokens. 

			A core building block of self-attention is the concept of queries, keys, and values. Each token in the input sequence is transformed into three vectors: a query vector, a key vector, and a value vector. These are computed as follows for a token \( t \):
			<br><br>
	  
			<center>
			<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>q</mi>
				  <mo>=</mo>
				  <msub>
					<mi>W</mi>
					<mi>q</mi>
				  </msub>
				  <mi>t</mi>
				</mrow>
			  </math>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>k</mi>
				  <mo>=</mo>
				  <msub>
					<mi>W</mi>
					<mi>k</mi>
				  </msub>
				  <mi>t</mi>
				</mrow>
			  </math>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>v</mi>
				  <mo>=</mo>
				  <msub>
					<mi>W</mi>
					<mi>v</mi>
				  </msub>
				  <mi>t</mi>
				</mrow>
			  </math>  
			</center>
			<br>

			The query vector represents the "question" a token asks about the sequence, while the key vectors encode the "context" or "relevance" of other tokens. The values represent the information each token contributes. To compute self-attention, the query of a token is compared against the keys of all tokens in the sequence using a similarity measure (dot product). This produces a vector of attention scores:
			<br><br>

			<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>s</mi>
				  <mo>=</mo>
				  <mfenced open="[" close="]">
					<mrow>
					  <mi>q</mi>
					  <msup>
						<mi>k</mi>
						<mo>T</mo>
					  </msup>
					</mrow>
				  </mfenced>
				</mrow>
			</math>
			<br>

			These attention scores are then normalized using a softmax function, converting them into probabilities that indicate the relative importance of each token:
			<br><br>

			<center>
				<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>a</mi>
				  <mo>=</mo>
				  <mi>softmax</mi>
				  <mo>(</mo>
				  <mi>s</mi>
				  <mo>)</mo>
				</mrow>
			  </math>
			  <br><br>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
				  <mi>output</mi>
				  <mo>=</mo>
				  <mi>a</mi>
				  <mi>v</mi>
				</mrow>
			  </math>
			</center>
			<br>

			The self-attention mechanism combines the values weighted by their attention probabilities, producing an output vector for each token that incorporates information from the entire sequence. This approach allows the model to capture both local and global dependencies, which is particularly powerful for tasks like machine translation and time-series prediction.

			Unlike traditional sequence models like recurrent neural networks (RNNs), self-attention does not impose a sequential constraint. Instead, it enables direct access to all tokens simultaneously, making it more efficient for parallel computation and better at handling long-range dependencies.

			In the following sections, we will explore how these principles underpin transformer architectures like GPT-2, focusing on their applications to complex prediction tasks in financial time series analysis.
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
			Adapted from lecture notes on Transformers architecture <a href="#ref_1">[1]</a>.
		</div>
		</div>
		<div class="content-margin-container" id="gpt2">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h4>GPT-2 Architecture</h4>

			For the prediction task we are trying to analyze we chose GPT-2 architecture, which builds upon the self-attention mechanism with a stack of transformer decoder blocks. Each block consists of the following key components:

			<ol>
				<li>
					<b>Masked Self-Attention:</b> GPT-2 uses masked self-attention to ensure that predictions for a token depend only on the tokens that precede it. This masking is achieved by setting the attention weights to zero for future tokens.
				</li>
				<li>
					<b>Feedforward Neural Network (FFN):</b> Each attention block is followed by a position-wise fully connected feedforward network with non-linear activation functions (ReLU or GELU). This component helps in learning complex mappings.
				</li>
				<li>
					<b>Layer Normalization:</b> Layer normalization is applied before the attention and feedforward layers, improving training stability and convergence.
				</li>
				<li>
					<b>Residual Connections:</b> Residual connections around each sub-layer (attention and feedforward) allow the model to effectively propagate gradients during training.
				</li>
			</ol>

			The input to GPT-2 is first embedded into a high-dimensional space using a token embedding layer and a positional encoding to provide the model with information about the order of tokens in the sequence. The embeddings are then passed through the stack of decoder blocks.

			<h4>GPT-2 for Autoregressive Prediction</h4>

			GPT-2 is trained in an autoregressive manner, predicting the next token in the sequence given all previous tokens. This is achieved by minimizing the negative log-likelihood of the predicted tokens during training. The model's autoregressive nature aligns well with financial time series prediction, where future values depend on past observations.

			By using multiple attention heads in each self-attention layer, GPT-2 can attend to different aspects of the input sequence simultaneously. This multi-head attention mechanism enables the model to learn diverse representations that are crucial for capturing the nuances of high-frequency financial data.

			In the next section, we will detail how we analyze the components of GPT-2 and their contribution to predictions in the context of financial time series data, utilizing the path patching framework inspired by Wang et al. (2022).
		    
			</div>
			<div class="margin-right-block" style="transform: translate(0%, -10%);"> 
				CITE HERE GPT-2 PAPER <a href="www.example.com">link</a>
			</div>
		</div>
	

			
		<div class="content-margin-container" id="task">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h1>Task definition</h1><p>
					We address an autoregressive prediction problem in the context of intraday financial markets. The objective is to predict the sign of the return 
					(<math xmlns="http://www.w3.org/1998/Math/MathML">
						<mrow>
							<mo>{</mo>
							<mn>1</mn>
							<mo>,</mo>
							<mn>0</mn>
							<mo>,</mo>
							<mo>-</mo>
							<mn>1</mn>
							<mo>}</mo>
						</mrow>
					</math>)
					for 30-minute intervals using a set of lagged input features. The task is framed as a three-class classification problem. To facilitate this, 
					we construct a comprehensive set of technical indicators derived from historical data, including log returns, volatility, and trading volume, 
					along with their lagged values. These features serve as the predictive inputs to the model. Below, we provide a schematic visualization of the input 
					representation and the prediction task.
				</p>
				<p>
					Below a visual representation of our model, where <b>X</b>, <b>Y</b>, <b>Z</b> are <i>(T x 1)</i> vectors, where <i>T</i> is our lookback period.
				</p>
				<center>
					<svg width="800" height="500" xmlns="http://www.w3.org/2000/svg">

						<!-- Connections -->
						<line x1="170" y1="150" x2="300" y2="150" stroke="gray" stroke-width="2" />
						<line x1="170" y1="250" x2="300" y2="250" stroke="gray" stroke-width="2" />
						<line x1="170" y1="350" x2="300" y2="350" stroke="gray" stroke-width="2" />
				
						<line x1="450" y1="150" x2="650" y2="250" stroke="gray" stroke-width="2" />
						<line x1="450" y1="250" x2="650" y2="250" stroke="gray" stroke-width="2" />
						<line x1="450" y1="350" x2="650" y2="250" stroke="gray" stroke-width="2" />
						
						<!-- Title -->
						<text x="290" y="30" font-size="18" font-family="Arial">Autoregressive Prediction</text>
						
						<!-- Input Layer -->
						<text x="70" y="100" font-size="16" font-family="Arial">Input Layer (Features)</text>
						
						<!-- Input Nodes -->
						<circle cx="150" cy="150" r="20" fill="lightgreen" />
						<text x="150" y="150" font-size="20" font-family="Arial" text-anchor="middle" dy=".3em">
							<tspan font-weight="bold">X</tspan>
						</text>
				
						<circle cx="150" cy="250" r="20" fill="lightgreen" />
						<text x="150" y="250" font-size="20" font-family="Arial" text-anchor="middle" dy=".3em">
							<tspan font-weight="bold">Y</tspan>
						</text>
				
						<circle cx="150" cy="350" r="20" fill="lightgreen" />
						<text x="150" y="350" font-size="20" font-family="Arial" text-anchor="middle" dy=".3em">
							<tspan font-weight="bold">Z</tspan>
						</text>
						
						<!-- GPT-2 Block -->
						<text x="275" y="100" font-size="16" font-family="Arial">Network (GPT-2 based architecture)</text>
						<rect x="300" y="120" width="150" height="300" fill="#d9e8ff" stroke="black" />
						<text x="375" y="270" font-size="14" font-family="Arial" text-anchor="middle">Network</text>
						
						<!-- Output Layer -->
						<text x="600" y="100" font-size="16" font-family="Arial">Output Layer</text>
						
						<!-- Output Node -->
						<circle cx="650" cy="250" r="20" fill="lightcoral" />
						<text x="650" y="250" font-size="20" font-weight="bold" font-family="Arial" text-anchor="middle" dy=".3em">Prediction</text>
						

					</svg>
				</center>
				<p>
					To implement our model, we adopted the <em>GPT-2 architecture</em>, tailored to the requirements of our autoregressive prediction task. Specifically, we initialized our model to accept input features shaped as 
					<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
						<mi>(B, T, N)</mi>
					</math>, 
					where 
					<em>B</em> represents the batch size, 
					<em>T</em> denotes the lookback period 
					(<math xmlns="http://www.w3.org/1998/Math/MathML"><mn>43</mn></math>, corresponding to three days of intraday data), 
					and <em>N</em> specifies the number of feature dimensions 
					(<math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn></math>, including lagged values of volatility, log returns, and volume).
				</p>
				<p>
					The input features were first processed through an <em>embedding layer</em> that projected the raw data into a higher-dimensional space, enabling the model to capture more intricate patterns in the data. To incorporate temporal information, a 
					<em>positional embedding layer</em> was added to encode the sequential nature of the time series, ensuring that the model could differentiate between input tokens based on their relative positions.
				</p>
				<p>
					The core of the network consists of three stacked <em>self-attention blocks</em>, adhering to the principles of the GPT-2 architecture. Each block includes:
					<ul>
						<li>A <em>multi-head self-attention mechanism</em> to capture dependencies across all lagged input features.</li>
						<li>A fully connected <em>feedforward network (MLP)</em> to process intermediate representations.</li>
						<li><em>Residual connections</em> with layer normalization to facilitate gradient flow and improve model stability during training.</li>
					</ul>
					These components work synergistically to extract meaningful relationships and patterns from the input data.
				</p>
				<p>
					In the final stage, the output of the self-attention blocks is passed through a <em>decoding layer</em> designed for classification. The decoder transforms the processed representations into predictions corresponding to one of the three target classes 
					(<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">{<mn>1</mn>, <mn>0</mn>, <mn>-1</mn>}</math>), 
					representing the sign of the return. This setup ensures that the model is well-suited for the autoregressive classification task, leveraging the strengths of the GPT-2 architecture for financial time series data.
				</p>
				<p>
					Below the visualization of the initialized model:
				</p>
				<img src="./images/GPT-2 Architecture.png" width=512px/>
			</div>
			<div class="margin-right-block" style="transform: translate(0%, -10%);">
				REFERENCE HERE <a href="www.example.com">link</a>
			</div>
		</div>

		<!-- NETWORK ANALYSIS PART  -->
		<div class="content-margin-container" id="net_analysis">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h1><p>Network Analysis</p></h1>
				
				  <p>
					Having defined the model we aim to train, the next step involves delving into its inner workings to make the model more interpretable and human-comprehensible. Our approach draws inspiration from the work of Wang et al. (2022), which introduces a mechanistic framework for understanding neural networks. One of the key concepts proposed is the <em>hook</em>. A hook is a dummy <code>nn.Module</code> that operates as a passthrough immediately after every activation function in the model, returning the identity of the data it processes. These hooks are instrumental in caching and analyzing the activations at various points in the model, particularly within attention layers, as well as conducting ablation studies.
				  </p>
				
				  <p>
					In our experiments, we leverage hooks to silence the outputs of attention heads selectively, aiming to identify which heads most significantly influence the model's predictions. This process employs a technique called <strong>mean ablation</strong>, where the output of a single head is replaced with its mean value. By iteratively deactivating different attention heads and observing changes in the loss, we can quantify the relative importance of each head. The comparison between loss variations enables us to rank heads in terms of their contribution to the prediction process.
				  </p>
				
				  <p>
					After identifying the most influential heads, we analyze their outputs to determine the temporal dependencies the model emphasizes. This allows us to uncover which time steps the model focuses on most heavily during its predictions. Such insights not only improve our understanding of the model's decision-making process but also provide a pathway to refine its interpretability and trustworthiness.
				  </p>
				
			</div>
			<div class="margin-right-block"></div>
		</div>

		<div class="content-margin-container" id="implementation">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h1> Implementation </h1>
				<p>
					We structured our implementation by dividing the process into four key steps: <b>1) Data Collection & Preparation</b>; <b>3) Model Training</b>; and <b>4) Network Analysis</b>.
				  </p>
				
				  <h4>1. Data Collection & Preparation</h4>
				  <p>
					In the initial phase, we sourced financial data from <a href="https://polygon.io" target="_blank">Polygon.io</a> by developing a bespoke API. This enabled us to retrieve comprehensive information on all U.S. stocks with 30-minute interval data spanning over five years. The high-resolution dataset provides a robust foundation for subsequent analysis and modeling. 
				  </p>
				  <p>
					Following data extraction, we implemented a series of rigorous data cleaning procedures to address potential issues that could adversely affect model training in subsequent phases. These procedures included the removal of missing values and duplicate entries, as well as the handling of outliers. Missing values were systematically addressed using interpolation techniques, 
					while duplicates were eliminated to maintain dataset integrity. Subsequently, we enriched the dataset by constructing additional features derived from the securities' price movements. Feature engineering focused 
					on key financial metrics such as logarithmic returns, rolling volatilities, and price-volume relationships. These features were selected due to their relevance in capturing temporal and cross-sectional patterns in the data. While the scope of feature engineering was constrained by time limitations, the chosen features provided a robust foundation for predictive modeling. 
					In preparing the dataset for model training, we structured the data into a format suitable for time-series classification tasks. Specifically, each sample in the training dataset was designed as an entry with the shape (<i>T</i>, <i>d</i> ), where <i>T</i> represents the temporal dimension and <i>d</i> denotes the number of features engineered. This configuration ensured
					that the model could effectively leverage both the temporal dynamics and the feature relationships present in the data. Finally, the dataset was partitioned into training and testing subsets to facilitate model evaluation. The temporal structure of the data was preserved during splitting to ensure that the test set consisted of future data relative to the training set, 
					thereby maintaining the chronological integrity necessary for time-series analysis. This division established the foundation for model training and validation in subsequent stages.
				  </p>
				
				  <h4>3. Model Training</h4>
				  <p>
					Talk about model training
				  </p>
				
				  <h4>4. Network Analysis</h4>
				  <p>
					Talk ABOUT MODEL TRAINING
				  </p>
			</div>
			<div class="margin-right-block"></div>
		</div>>


		<div class="content-margin-container" id="results">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h1>Results</h1>
				<p>
					Following our network analysis proceess, 2 main results come to right.
				</p>

				<h4> SMALL MODEL</h4>
				<p>
					Below the heat map for the first model
				</p>
				<img src="./images/heat_1.png" width=512px/>
				<br>
				<p>
					Comment
				</p>
				<p>
					Below the importance for the first model
				</p>
				<img src="./images/importance_1.png" width=512px/>
				<br><br>
				<h4> BIG MODEL</h4>
				<p>
					Below the heat map for the second model
				</p>
				<img src="./images/heat_2.png" width=512px/>
				<br>
				<p>
					Comment
				</p>
				<p>
					Below the importance for the first model
				</p>
				<img src="./images/importance_2.png" width=512px/>
			</div>
			<div class="margin-right-block"></div>
		</div>>


		<!-- Conclusion -->
		<div class="content-margin-container" id="conclusion">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
				<h1>Conclusion</h1>
			</div>
			<div class="margin-right-block"></div>
		</div>


		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Another section</h1>
            In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video>
		    </div>
		    <div class="margin-right-block">
					A caption for the video could go here.
		    </div>
		</div>



		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
							<a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>
